\section{Probabilistic Modeling}
Suppose we have access to \(\P_{XY}\) then opt. sol.:
\textit{Reg, (SE):} \(\hat{f}(x) = \E[Y \mid X=x]\), \(Y = f^*(X) + \epsilon\)
\textit{C.\(_{0-1}\):} \(\hat{f}(x) = \P_{Y \mid X}(Y \neq \text{sgn} f(X))\), \(y = \epsilon y^*(x)\)

Get \(\P(Y \mid X)\) from \(\P(X, Y)\), but not vice versa.

\begin{definition}[Naive \(\P_{XY}\) Est.]
  Kernel density est./histogram
\end{definition}

\subsection*{Parametric Models for \(\P_{XY}\)}
Best of distribution family \(\mathcal{P} = \{\P_{XY}; \theta \in \Theta\}\)

\begin{definition}[MLE]
  Likelihood: \(p(D; \theta) = \prod p(x_i; \theta)\) with its estimator \(\theta_{\text{MLE}} = \argmax \log p(D; \theta)\).
\end{definition}

\subsection[Discriminative]{Discriminative \(p(x, y) = p(y \mid x ; \gamma) p(x; \pi)\)}

\textbf{Ex. Reg.} \(X \sim \normal(\mu, 1), \P_{Y \mid x;w} = \N(w^\top x, 1)\):
\begin{enumerate}
  \item \(\hat{\mu}_{\text{MLE}} = \frac{1}{n}\sum x_i\) as sample mean for \(\P_X\).
  \item \(\hat{w}_{\text{MLE}} = \argmin \sum(y_i - w^\top x_i)^2\) for \(\P_{Y \mid x; w}\).
  \item \(\hat{p}(x, y) = p(x; \hat{\mu}_{\text{MLE}}) \cdot p(y \mid x; \hat{w}_{\text{MLE}})\)
\end{enumerate}

\textbf{Ex. Cl.} \(X \sim \normal(\mu, 1), p(y \mid x; w) = \sigma(y w^\top x)\).
\begin{enumerate*}
  \item \(\mu = \hat{\mu}_{\text{MLE}}\)
  \item \(\hat{w}_{\text{MLE}} = \argmin \sum g_{\text{log}}(y_iw^\top x_i)\)
\end{enumerate*}

\subsection[Generative]{Generative \(p(x, y) = p(x \mid y; \gamma) p(y; \pi)\)}
\textbf{Setup Ex.} \(Y \sim \cat(\pi), \P_{X \mid y; \mu_y, \Sigma_y} \sim \normal(\mu_y, \Sigma_y)\) with \(\pi \in \Pi, \Sigma_y \in S\) and \( y \in \{1, 2\}\).
\[p(x \mid y; \mu_y, \Sigma_y) = \frac{1}{(2\pi)^{\frac{d}{2}}(\det \Sigma_y)^{\frac{1}{2}}} e^{- \frac{(x - \mu_y)^\top \Sigma_y^{-1} (x - \mu_y)}{2}}\]

\begin{definition}[Gaus. Na√Øve Bayes]
  \(\Sigma_y = \text{diag}[\sigma_{y,1}^2, \sdots, \sigma_{y,d}^2]\).
  \begin{enumerate*}
    \item \([\hat{\pi}]_j = \hat{p_j} = \frac{\#\{Y = j\}}{n}\)
    \item \(\hat{\mu}_y = \frac{1}{\#\{Y = y\}} \sum_{i:y_i = y} x_i\)
    \item \(\hat{\sigma}_{y, k} = \frac{1}{\#\{Y = y\}} \sum_{i:y_i = y} (x_{i, k} - \mu_{y, k})^2\) w/ MLE.
  \end{enumerate*}
  GNB performs better for small sample sizes. Has correct uncertainty for big samples. If iid:
  \(\hat{y} = \argmax p(y \mid x) = \argmax p(y) \prod p(x_i \mid y)\).
\end{definition}

\begin{definition}[Linear Discriminant Analysis]
  \(\forall y: \sigma_y = \sigma\)
\end{definition}

\pagebreak
\begin{definition}[Gaussian Bayes Classifiers]
  Same as GNB, but \(\hat{\Sigma}_y = \frac{1}{\#\{Y = y\}}\sum_{i:y_i = y} (x_i - \hat{\mu}_y)(x_i - \hat{\mu}_y)^\top\).
  
  Also called quadratic discriminant analysis.
\end{definition}

\subsection{Bayesian Modeling}
Ass. data iiid. from \(\P_{\cdot \mid \theta}\) with prior distribution \(\theta \sim \P_\theta\). Then \(p(D) = \int p(D \mid \theta) p(\theta) \, d\theta\).

\begin{definition}[MAP]
  Posterior: \(p(\theta \mid D) = \frac{p(D \mid \theta) p(\theta)}{\int p(D \mid \theta) p(\theta) \, d\theta}\) \&
  \[\hat{\theta} = \argmax \log p(\theta \mid D) = \argmax \log p(D, \theta)\]
\end{definition}

\begin{definition}[Ex. Reg.]
  \(y_i = w^\top x_i + \epsilon_i\), \(w \sim \normal(0 , \sigma_w^2 I_d)\), \(\epsilon \sim \normal(0, 1)\), \(\mathcal{P} = \{\P_{Y \mid X; w} = \normal(\langle w, x\rangle, 1)\}\).
  \[\hat{w}_{\text{MAP}} = \argmin \frac{1}{2}||y- Xw||_2^2 + \frac{1}{2\sigma_w^2} ||w||^2\]
  \(=\) ridge sol. If \(p(w) = \frac{1}{z}e^{-\frac{||w||_1}{\sigma_w}}\) laplacian then
  \[\hat{w}_{\text{MAP}} = \argmin \frac{1}{2}||y- Xw||_2^2 + \frac{1}{\sigma_w} ||w||_1\]
  which is the lasso sol. \(\to \hat{\P}_{Y \mid X} = \P_{Y \mid X; \hat{w}_{\text{MAP}}}\).
\end{definition}

\begin{definition}[Bayes. Model Avg]
  Gives distribution of \(f^*\):
  \begin{align*}
    \hat{p}(y \mid x; D) &= \hat{E}_{\theta \mid D}p(y \mid x; \theta) \\
    &= \int_\Theta p(y \mid x; \theta) \hat{p}(\theta \mid D) \, d\theta
  \end{align*}
\end{definition}

\subsection{Decision Theory}
Decision rules \(a : X \to A\), with \(A\) as action set.
Find \(a^*(x) = \argmin \hat{\E}[l(a(x), y) \mid X = x]\)

\begin{definition}[Applications of decision theory w/ \(\P(Y \mid X)\)]
  \begin{itemize*}
    \item Reg. SE: \(\hat{f}(x) = \argmin_a \hat{\E}[(Y - a)^2 \mid X=x]\) \(= \hat{E}[Y \mid X = x]\)
    \item 0-1: \(\hat{y}(x) = \argmax_y \hat{p}(y \mid x) \) \(= \argmin_a \hat{E}[\I_{a \neq Y} \mid X = x]\)
    \item a0-1: Boundary \(\pi(x)\) to \(\pi(x) = \frac{c_{FN}}{c_{FP} + c_{FN}}\)
    \item Abstention 0-1: with \(A = \{-1, +1, r\}\) and \(l(\hat{y}, y) = \I_{\hat{y} \neq y}\I_{\hat{y} \neq r} + c \I_{\hat{y} = r}\) obtain \(\hat{y} = r\) if \(c < \hat{p}(y = -1 \mid x) < 1 - c\).
  \end{itemize*}
\end{definition}

\subsection{Summary (Gen. Classification)}
\begin{enumerate*}
  \item Est. \(p(y)\)
  \item Est. \(p(x \mid y)\)
  \item Obtain \(p(y \mid x)\)
\end{enumerate*}
\begin{align*}
  \hat{y} &= \argmax_y p(y \mid x) \\ &= \argmax \log p(y) + \log(x \mid y)
\end{align*}