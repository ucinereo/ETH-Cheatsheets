\section{Neural Networks}
\begin{definition}[Activation Function]
  \(\phi(x; w) = \varphi(w^\top x)\)
\end{definition}

\begin{itemize*}
  \item \textbf{tanh}: \(\frac{\exp(z) - \exp(-z)}{\exp(z) + \exp(-z)}\)
  \item \textbf{relu}: \(\max\{0, z\}\)
  \item \(\sigma(z)\)
\end{itemize*}

\begin{definition}[Universal Approx. Thm.]
  \(\forall \epsilon_{>0}, \exists\)neural network that approximates any function within \(\epsilon\).
\end{definition}

\subsection{Forward Propagation}
  \textit{Input l.}: \(v^{(0)} = [x; 1]\)
  \textit{Output l.}: \(f = W^{(L)}v^{(L-1)}\)
  \textit{Hidden l.}: \(z^{(l)} = W^{(l)}v^{(l-1)}\) \& \(v^{(l)} = [\varphi(z^{(l)}); 1]\)

\subsection{Backward Propagation}
\textcolor{H1}{Given from L+1}, \textcolor{H2}{to compute}, \textcolor{H3}{given from FP}.
\begin{gather*}
  (\nabla_{W^(L)}l)^\top =\textcolor{H2}{\frac{\partial l}{\partial f}}\textcolor{H3}{\frac{\partial f}{\partial W^{(L)}}} = \textcolor{H2}{\frac{\partial l}{\partial f}}\textcolor{H3}{v^{(L-1)}} \\
  (\nabla_{W^(L-1)}l)^\top = \textcolor{H1}{\frac{\partial l}{\partial f}} \textcolor{H2}{\frac{\partial f}{\partial z^{(L-1)}}}\textcolor{H3}{\frac{\partial z^{(L-1)}}{\partial W^{(L-1)}}} = \textcolor{H1}{\sdots} \textcolor{H2}{\sdots}\textcolor{H3}{v^{(L-2)}}\\
  (\nabla_{W^(L-2)}l)^\top = \textcolor{H1}{\frac{\partial l}{\partial f} \frac{\partial f}{\partial z^{(L-1)}}} \textcolor{H2}{\frac{\partial z^{(L-1)}}{\partial z^{(L-2)}}}\textcolor{H3}{\frac{\partial z^{(L-2)}}{\partial W^{(L-2)}}}
\end{gather*}
Where error \(\delta^{(l)} = \varphi(z^{(l)}) \odot (W^{(l+1)\top} \delta^{(l_1)})\) \\ and \(\nabla_{W^{(l)}}l = \delta^{(l)}v^{(l-1)\top}\) to calc the gradient.

\subsection*{Overfitting and Robustness}
To avoid \(0, *\) grad. keep \(\Var\) of activation const.
Init \(W\): tanh: \(\normal(\frac{1}{n_{in}} \, \text{or} \, \frac{2}{n_{in} + n_{out}})\); relu: \(\normal(\frac{2}{n_{in}})\).

\textbf{LR}: piecewise constant \(\searrow\) or w/ momentum.
\textbf{Prevent Overfitting}: \begin{itemize*}
  \item Dropout(Eval \(\hat{w} = wp\))
  \item Regularization
  \item Normalization
  \item Early Stop
\end{itemize*}

\subsection{CNN and other architectures}
Use \(v^{(l+1)} = \varphi(W * v^{(l)})\). Output of \(m\) different \(f \times f\) filter to an \(n \times n\) image with padding \(p\) and stride \(s\): \(l \times l \times m\) with \(l = \frac{n + 2p - f}{s} +1\).

\begin{definition}[Pooling Layers]
  Pool units to decrease width.
\end{definition}

\begin{definition}[ResNet]
  \(v^{(l + 1)} = v^{(l)} + r(v^{(l)})\) w/ skip conn.
\end{definition}
