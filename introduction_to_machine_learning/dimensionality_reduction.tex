\section{Dimensionality Reduction} \vspace{-11pt}
\begin{gather*}
  w^* = \argmin_{w, z, ||w||_2 = 1} \sum_i ||x_i - w z_i||_2^2 \\
  z_i^* = w^\top x_i \implies w^* = \argmin_{||w||_2 = 1} w^\top \Sigma w
\end{gather*}
With \(\Sigma = \frac{1}{n} \sum_i x_i x_i^\top\) as the empirical covariance matrix (assuming \(\mu = 0\)). Solution given by principal eigenvector of \(\Sigma\).

\begin{definition}[PCA problem (\(k > 1\))]
  \(w \to W\) s.t. \(W^\top W = I\), \(W = [v_1 | \ldots | v_k]\) the \(k\)-first eigenvectors of \(\Sigma\).

  Repr. \(z_i = W^\top x_i\). Recon. \(\tilde{x}_i = W W^\top x_i\)
\end{definition}

\begin{definition}[PCA via SVD]
  \(X = U \Sigma V^\top \to W = V_{:, 1:k}\)
\end{definition}

\begin{definition}[Kernelized PCA]
  With \(w = \sum \alpha_j \phi(x_j)\) and
  \begin{gather*}
    \argmax_{||w|| = 1} w^\top \Sigma w = \argmax w^\top X^\top X w \implies \\
    \alpha^* = \argmax_\alpha \frac{\alpha^\top K^\top K \alpha}{\alpha^\top K \alpha}
  \end{gather*} 
  With closed form solution (for any \(k\)): \\
  \(\alpha^{(i)} = \frac{1}{\lambda_i} v_i\) from \(K = \sum_i \lambda_i v_i v_i^\top, \lambda_1 \geq \sdots \geq \lambda_n\).
  \(\implies z_i = \sum_j \alpha_j^{(i)} k(x_j, x)\) as projection.
\end{definition}

\begin{definition}[Autoencoder]
  \(W^* = \argmin \sum ||x_i - f_W(x_i)||_2^2\)
  Thus \(f(x; \theta) = f_{\text{dec}}(f_{\text{enc}}(x ; \theta_{\text{enc}}); \theta_{\text{dec}})\) and if activation is identity and square loss \(\equiv\) PCA.
\end{definition}