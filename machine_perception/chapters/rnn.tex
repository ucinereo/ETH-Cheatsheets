\section{RNN}

Hidden state summarizing previous input. Learn \textbf{one} model:

\(\h^t = f(\h_{t-1}, x_t; \p)\): \(f\) transition f. Same \(f\) and \(\p\) over time.

\begin{definition}[Vanilla RNN]
    \(\h_t = \tanh(\W_{hh}\h_{t - 1} + \W_{xh}\x_t + b), \hat\y_t = \W_{hy} \h_t\).
    \begin{itemize*}
        \item \(\tanh\) because 0-center and higher grad norm \(\implies\) faster conv.
        \item Reparam: \(\h_t = \W_{hh} f(\h_{t-1}) + \W_{xh}\x_t + b\), \(L_t = \norm{\hat \y_t - \y_t}^2\)
    \end{itemize*}
\end{definition}

\begin{definition}[BPTT]
    \(\frac{\partial L}{\partial \W} = \sum_{t=1}^S \frac{\partial L_t}{\partial \W}\), 
    \(\frac{\partial \L_t}{\partial \W} = \sum_{k=1}^t \frac{\partial L_t}{\partial \hat\y_t} \frac{\partial \hat\y_t}{\partial \h_t} \frac{\partial \h_t}{\partial \h_k} \frac{\partial^+ \h_k}{\partial \W}\)

    \(\frac{\partial \h_t}{\partial \W} = \frac{\partial^+ \h_t}{\partial \W} + \frac{\partial \h_t}{\partial \h_{t-1}}\frac{\partial \h_{t-1}}{\partial \W} = \sum_{k=1}^t\frac{\partial \h_t}{\partial \h_k} \frac{\partial^+\h_k}{\partial \W}\)
    
    \(\frac{\partial \h_t}{\partial \h_k} = \prod_{i = k+1}^t \frac{\partial \h_i}{\partial \h_{i-1}} = \prod_{i = k+1}^t \W_{hh}^\top \text{diag}[f'(\h_{i-1})]\)
\end{definition}

\begin{definition}[Exploding/Vanishing Grad]
    Because \(\sigma'\) and \(\tanh'\) bounded, \(\norm{\text{diag}[f'(h_{i-1})]} < \gamma\) for some \(\gamma\). Then if \(\lambda_1 < \frac{1}{\gamma}\), we have vanishing and otherwise exploding gradients over many time steps.
\end{definition}

\begin{definition}[Trauncated BPTT]
    \(\frac{\partial L_t}{\partial \W} \approx \sum_{k = t - \kappa}^t \frac{\partial L_t}{\partial \hat\y_t} \frac{\partial \hat\y_t}{\partial \h_t}\frac{\partial \h_t}{\partial h_k}\frac{\partial^+ \h_k}{\partial \W}\).
\end{definition}

\begin{definition}[Gradient Clipping]
    If \(\norm{\vb{g}} \geq\), then \(\vb{g} = \frac{\tau}{\norm{\vb{g}}} \vb{g}\).
\end{definition}

\begin{definition}[LSTM]
Keep separate memory cell (state) with gated (protected) access. Can forget. All learned. Computationally expensive. GRU presents a more efficient simplification.
Input Gate (which values to write), Forget Gate (which values to forget), Output Gate (which values to output), Gate Gate (candidate values).
\[
\mathbf{
    \begin{bmatrix}
        \mathbf{i}_t \\
        \mathbf{f}_t \\
        \mathbf{o}_t \\
        \mathbf{g}_t
    \end{bmatrix}
} = 
\begin{bmatrix}
    \sigma \\
    \sigma \\
    \sigma \\
    \tanh
\end{bmatrix}
\left( W
\begin{bmatrix}
    \mathbf{x}_t {\color{gray}\h_t\l{l-1}}\\
    \mathbf{h}_{t-1} {\color{gray}\h_{t-1}\l l}
\end{bmatrix}
\right)
\quad
\begin{aligned}
\mathbf{c}_t &= \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \mathbf{g}_t \\
\mathbf{h}_t &= \mathbf{o}_t \odot \tanh(\mathbf{c}_t)
\end{aligned}
\]

\end{definition}
