\section{Estimators}
We make the following assumptions:
\begin{itemize}
  \item Parameter space \(\Theta \subseteq \R^m\)
  \item Family of probability distributions \(\{\P_\theta \mid \theta \in \Theta\}\) on the sample space \((\O, \F)\); for each element \(\theta = (\theta_1, \ldots, \theta_m)\) in the parameter space, there exists a probability model \(\P_\theta\), which describes the relationship between the data and the unknown parameters.
  \item Random variables \(X_1, \ldots, X_n\) on \((\O, \mathcal{F})\)
  \item We refer to the observed data \(x_1, \ldots, x_n\) or the random variables \(X_1, \ldots, X_n\) as a sample.
  \item For each \(\theta_1, \ldots, \theta_m\) we search for an estimator \(T_j\).
\end{itemize}

\begin{definition*}[Estimator]
  An estimator is a random variable \(T_j: \O \to \R\) of form
  \[T_j = t_j(X_1, \ldots X_n) \qquad t_j: \R^n \to \R\]
\end{definition*}

\textbf{Notation}: \(T = (T_1, \ldots, T_m)\) and \(\theta = (\theta_1, \ldots, \theta_m)\).

\begin{definition*}[Unbiased]
  An estimator \(T\) is unbiased, if \(\forall \theta \in \Theta: \E_\theta(T) = \theta\).
\end{definition*}

\begin{definition*}[Bias]
  The bias of \(T\) in the model \(\P_\theta\) is defined as \(\E_\theta(T) - \theta\).
\end{definition*}

\begin{definition*}[Consistent]
  The sequence \(T^{(n)}\) is \textbf{consistent} if \(T^{(n)} \stackrel{\P_\theta}{\to}\theta\) as \(n \to \infty\).
\end{definition*}

\begin{definition*}[Mean Squared Error (MSE)]
  \begin{align*}
    \MSE_\theta(T) &= \E_\theta((T-\theta)^2) \\
    &= \Var_\theta(T) + (\E_\theta(T) - \theta)^2
  \end{align*}
\end{definition*}

An estimator $T$ gives us a random single value for $\theta$, drawn from the sampling distribution of the estimator, which represents the distribution of possible values that we would get if we were to calculate the estimator for many different samples of data.

\subsection{Maximum-Likelihood-Method}
Let \(X_1, \ldots, X_n\) be random variables on \(\P_\theta\). The \textbf{Likelihood-function} \(L: \Theta \to \R\) is defined as:
\[L(x_1, \ldots, x_n ; \theta) = \begin{cases}
  p(x_1, \ldots, x_n ; \theta) & \text{discrete case} \\
  f(x_1, \ldots, x_n ; \theta) & \text{continuous case}
\end{cases}\]
with \(p\) (or \(f\)) being the joint PMF/PDF.
Then for each \(x_1, \ldots, x_n \in \DX\) let \(t_{\ML}(x_1, \ldots, x_n)\) be the value which maximizes the function \(\theta \mapsto L(X_1, \ldots, X_n ; \theta)\).
The \textbf{Maximum-Likelihood-Estimator} is then defined as
\[T_{\ML} = t_{\ML}(X_1, \ldots, X_n) \in \underset{\theta}{\arg\max} \, L(X_1, \ldots, X_n ; \theta).\]

\begin{note*}[Alternative notation of MLE (from IML)]
  \[\hat{\theta}_{\MLE} = \underset{\theta}{\arg\max} \, L(\theta; X, y) = p(y_1, \ldots, y_n \mid x_1, \ldots, x_n, \theta)\]
\end{note*}

\subsection{Method of Moments}
\begin{definition*}[$k$-th sample moment]
  \[\hat{\mu_k} = \frac{1}{n} \sum_{i=1}^n x_i^k\]
\end{definition*}

Suppose we know the formula of the first \(k\) moments \(\mu_1 = f_1(\theta_1, \ldots, \theta_n)\), \(\ldots\), \(\mu_n = f_n(\theta_1, \ldots, \theta_n)\), then the method of moments estimator for \(\theta_1, \ldots, \theta_n\) is denoted by \(\hat{\theta_1}, \ldots, \hat{\theta_n}\) and can be solved with the following system of equations:
\[\hat{\mu_1} = f_1(\hat{\theta_1}, \ldots, \hat{\theta_n}) \quad \ldots \quad \hat{\mu}_n = f_n(\hat{\theta_1}, \ldots, \hat{\theta_n})\]

\subsection{Bayesian Inference}
Instead of making a point estimator, Bayes' inference provides a full posterior distribution that captures uncertainty in parameter estimation.

We assume \(\theta \sim \pi_0\) (\(\P(\theta = t) = \pi_0(t)\)) as \textbf{prior}. Then we can calculate the \textbf{posterior distribution}:
\begin{align*}
  \P(\theta \mid x_1, \ldots, x_n) &= \frac{\P(x_1, \ldots, x_n, \theta)}{\P(x_1, \ldots, x_n)}\\
  &= \frac{\P(x_1, \ldots, x_n \mid \theta) \pi_0(\theta)}{\int f(x_1, \ldots, x_n \mid \theta = \theta')\pi_0(\theta') \, d\theta'}
\end{align*}
