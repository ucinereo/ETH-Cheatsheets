\section{Probability Stuff}

% \begin{colored}
%     \textbf{Expectation Dump}

%     \begin{itemize}
%         \item \(\E[\vb{A}\X + \vb{B}\Y + \vb{b}] = \E[\vb{A}\X] + \E[\vb{B}\Y] + \vb{b}\)
%         \item \(\E[\X\Y^\top] = \E[\X] \E[\Y]^\top\) if \(\X \bot \Y\)
%         \item \textbf{LOTUS} \(\E[f(\X)] = \int_{\X(\Omega)} f(\x)p(\x)\,d\x\)
%         \item \textbf{TOWER} \(\E_{\Y} [\E_{\X}[\X \mid \Y]] = \E[\X]\)
%     \end{itemize}
% \end{colored}

\begin{colored}
    % \textbf{Expectation}
        % \(\E[\vb{A}\X + \vb{B}\Y + \vb{b}] = \E[\vb{A}\X] + \E[\vb{B}\Y] + \vb{b}\)
    \(\E[\X\Y^\top] = \E[\X] \E[\Y]^\top\) if \(\X \bot \Y\)
    \(\E[f(\X)] = \int f(\x)p(\x)\,d\x\)
    \textbf{TOWER} \(\E_{\Y} [\E_{\X}[\X \mid \Y]] = \E[\X]\)
\end{colored}

\begin{colored}
    \textbf{Ind.}
    \(\X\bot\Y \iff p(\x\y) = p(\x)p(\y) \iff p(\x \mid y) = p(\x) \)
    % \begin{align*}
    \(\X\bot\Y\mid\Z \iff p(\x, \y \mid \z) = p(\x\mid\z) p(\y \mid \z)
    \iff p(\x \mid \y, \z) = p(\x \mid \z)\)
    % \X\bot\Y\mid\Z &\iff p(\x, \y \mid \z) = p(\x\mid\z) p(\y \mid \z) \\
    % &\iff p(\x \mid \y, \z) = p(\x \mid \z)
    % \end{align*}

    % Reichenbach: \(\forall\X, \Y \exists \Z: \X \not\bot \Y \implies \X \bot \Y \mid \Z\)
\end{colored}

\begin{colored}
    \(\Cov[\X, \Y] = \E[(\X - \E[\X])(\Y - \E[\Y])^\top]\);
    \(\Var[\X + \Y] = \Var[\X] + \Var[\Y] + 2 \Cov[\X, \Y]\);
    \textbf{LOTV} \(\Var[\X] = \E_{\Y}[\Var_{\X}[\X \mid \Y]] + \Var_{\Y}[\E_{\X}[\X \mid \Y]]\);
    \(\Var[\vb{A}\X + \vb{b}] = \vb{A}\Var[\X]\vb{A}^\top\)
\end{colored}

\begin{colored}
    \begin{itemize*}
        % \item \(p(x_{1:i-1}, x_{i+1,n}) = \int_{X_i(\Omega)}p(x_{1:i-1}, x_i, x_{i+1:n}) \,dx_i\)
        \item \textbf{Chain} \(p(x_{1:n}) = p(x_1)\cdot \prod_{i=2}^n p(x_i \mid x_{1:i-1})\) \\
        \item \textbf{LOTP} \(p(\x) = \int_{\Y(\Omega)} p(\x \mid \y) \cdot p(\y) \, d\y\) \\
        \item \(\Y = f(\X) : p(\y) = p_{\X}(f^{-1}(\y)) \cdot |\det Df^{-1}(\y)|\)
    \end{itemize*}
\end{colored}

\begin{colored}
    \textbf{\color{H3} Gauss.}
    \(p(\x) = \frac{\exp\left(-\frac{1}{2}(\x - \m)^\top \S^{-1}(\x - \m)\right)}{(2 \pi)^{n/2} \sqrt{\vert\S\vert}}\)
    % \begin{align*}
    %     \X_A \mid \X_B &= \x_b \sim \Normal (\m_{A \mid B}, \S_{A \mid B}) \ \text{where} \\
    %     \m_{A \mid B} &= \m_A + \S_{AB}\S_{BB}^{-1}(\x_B - \m_B),\\
    %     \S_{A \mid B} &= \S_{AA} - \S_{AB}\S_{BB}^{-1}\S_{BA}.
    % \end{align*}
        \(\X_A \mid \X_B = \x_b \sim \Normal (\m_{A \mid B}, \S_{A \mid B})\)
        where
        \(\m_{A \mid B} = \m_A + \S_{AB}\S_{BB}^{-1}(\x_B - \m_B)\), 
        \(\S_{A \mid B} = \S_{AA} - \S_{AB}\S_{BB}^{-1}\S_{BA}.\)

    % \begin{itemize*}
    %     \item Mean affine f. of \(\m_B\)
    %     \item \(\Var\) can only shrink and does not depend on the observations \(\x_B\).
    % \end{itemize*}

    Equivalently, if \(\X_A\), \(\X_B\) jointly gaussian, then
        \(\X_A = \vb{A}\X_B + \vb{b} + \vb*{\epsilon}\), where
        \(\vb{A} = \S_{AB}\S_{BB}^{-1}\), \\
        \(\vb{b} = \m_A - \S_{AB}\S_{BB}^{-1}\m_B\) and 
        \(\vb*{\epsilon} = \Normal(0, \S_{A \mid B})\)
\end{colored}

\subsection{Bayesian Learning (conditioned on \(\X\))}
\vspace{-8pt}

\[\overbrace{p(\theta \mid \X, \y)}^{\text{Posterior}} = \frac{\overbrace{p(\theta)}^{\text{Prior}} \overbrace{\prod p(y_i \mid \x_i, \theta)}^{\text{Likelihood} \ (p(\y \mid \X, \theta))}}{\underbrace{p(\y \mid \X) ({\scriptscriptstyle = \int p(\theta) \prod p(y_i \mid \x_i, \theta) \, d\theta})}_{\text{Marginal Likelihood ({\color{H4} mostly intracable})}}}\]

\begin{enumerate*}
    \item Define Prior and LL (BLR, GPR)
    \item Predict: \\ \(p(y^\star \mid \x^\star, \X, \y) = \int p(y^\star \mid \x^\star, \theta) p(\theta \mid \X, \y) \, d\theta\)
\end{enumerate*}
\begin{center}
\color{H1}For BLR and GPR, this has a closed form!
\end{center}

