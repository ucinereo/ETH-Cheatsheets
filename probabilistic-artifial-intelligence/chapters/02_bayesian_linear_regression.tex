\section{Bayesian Linear Regression}

\textbf{Idea}: Reason \(\w\)'s \textit{full posterior}, not only mode.

\begin{definition}[Model]
    \(y_i = {\w^\star}^\top \x_i + \epsilon_i\) with \(\epsilon_i \sim \Normal(0, \sigma_n^2)\) or equivalently \(y_i \mid \x_i, \w \sim \Normal(\w^\top \x_i, \sigma_n^2)\).
    With this model we find \(\hat\w_{\text{MLE}}  = \hat\w_{\text{ls}}\).
\end{definition}

\begin{colored}
    \textbf{Ridge Interpretation}: Ass. \(\w \sim \Normal(\0, \sigma_p^2\I)\), \(\w \bot \x_{1:n}\), \(y_i \bot y_j \mid \x_{1:n}, \w\). Then log-posterior is
    \(\log p(\w \mid \x_{1:n}, y_{1:n}) = -\frac{1}{2} [\w^\top \S \w - 2\m] + \text{const}\) \\
    w/ \(\S = (\sigma_n^{-2}\X^\top\X + \sigma_p^{-2}\I)^{-1}\), \(\m=\sigma_n^{-2}\S\X^\top\y\).
    Note we used \(p(\w \mid \y, \X) = \frac{p(\y \mid \X, \w)p(\w)}{p(\y\mid\X)}\), where we condition everywhere on \(\X\), but \(\w \bot \X\).
    \textbf{MAP}: \(\hat\w_{\text{MAP}} = \text{argmin}_{\w} \norm{\y - \X\w}_2^2 + \frac{\sigma_n^2}{\sigma_p^2}\norm{\w}_2^2\).
    \(\hat\w_{\text{ridge}} = (\X^\top\X + \lambda \I)^2 + \lambda\norm{\w}_2^2 \quad (\X\in\R^{n \times d})\)
    Note if \(\w \sim \Laplace(\0, l) \implies \lambda = \sigma_n^2/l\).
\end{colored}

MAP approximates full posterior by placing all mass on its mode, BLR predicts by averaging.

\begin{definition}[Inf.]
    \resizebox{0.88\linewidth}{!}{\(y^\star \mid \x^\star, \x_{1:n}, y_{1:n} \sim \Normal(\mu^\top \x^\star, {x^\star}^\top \S \x^\star + \sigma_n^2)\)}

    \resizebox{\linewidth}{!}{\(\Var[y^\star \mid \x^\star] = \underbrace{\E_\theta[\Var_{y^\star}[y^\star \mid \x^\star, \theta]]}_{\text{Aleatoric (Data)}} + \underbrace{\Var[\E_{y^\star}[y^\star \mid \x^\star, \theta]]}_{\text{Epistemic (Model)}}\)}
\end{definition}

\subsection{Kernelized BLR}
\begin{definition}[Kernelized BLR] (\(\Phi = \phi(\X)\), prior \(\w \sim \Normal\))
    \(\f \mid \X \sim \Normal(\Phi \E[\w], \Phi\Var[\w]\Phi^\top) = \Normal(\0, \K)\), with \(\K = \sigma_p\Phi\Phi^\top\), hence \(k(\x, \x') = \Cov[f(\x), f(\x')]\).
\end{definition}

Conditions for a valid kernel function \(k\): \\
\begin{itemize*}
  \item \(k(x, z) = k(z, x)\)
  \item \(K\) psd s.t. \(\forall x. \ x^\top K x \geq 0 \)
\end{itemize*}

\begin{definition}[Inner Product kernel]
  \(k(x, z) = h(\langle x, z \rangle)\)
\end{definition}

\begin{definition}[Poly ker.]
  \(k(x, z) = (c_{\geq 0} + \langle x, z \rangle)^m\), \(d_\phi = \binom{d + m}{d}\)
\end{definition}

\begin{definition}[RFB kernel]
  \(k(x, z)  = \exp\left(\frac{||x - z||_2^\alpha}{\tau}\right)\) which is \textbf{Gaussian} \(: \alpha = 2\), \textbf{Laplacian} \(: \alpha = 1\). \(d_\phi = \infty\)
\end{definition}

\begin{definition}[Kernel Composition]
  \begin{itemize*}
    \item \(k_1 \dotplus k_2\)
    \item \(c\cdot k \ (c >0)\)
    \item \(k((x \ y), (x' \ y')) = k_1(x \ x') \dotplus k_2(y \ y') \)
    \item \(f(k)\) for any poly. \(f\) with positive coeff or \(f = \exp\)
  \end{itemize*}
\end{definition}

\begin{definition}[Stationary]
  \(\iff \exists \tilde k: \tilde k (\x - \x') = k(\x, \x')\)
\end{definition}

\begin{definition}[Isotropic]
  \(\iff \exists \tilde k: \tilde k (\norm{\x - \x'}_2) = k(\x, \x')\)
\end{definition}

